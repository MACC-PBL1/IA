{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to detect data exfiltration using clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Standard Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import (\n",
    "    dataclass,\n",
    "    replace,\n",
    ")\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    Literal,\n",
    ")\n",
    "import logging\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars import selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Set Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. User Modifiable Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset file to be downloaded/used\n",
    "DATASET_ROOT: Path = Path(\"data\")\n",
    "DATASET_PATH: Path = DATASET_ROOT / \"dataset.csv\"\n",
    "\n",
    "# S3 Configuration \n",
    "USE_S3: bool = False \n",
    "S3_BUCKET_NAME: str = \"logs-pobl\"\n",
    "S3_DATASET_KEY: str = \"merged-logs/merged.csv\"  \n",
    "S3_MODELS_PREFIX: str = \"production_models/\" \n",
    "LOW_VARIANCE_THRESHOLD: float = 0.01 \n",
    "\n",
    "PARAMETER_TUNING: bool = False\n",
    "\n",
    "# Log file path\n",
    "LOG_PATH: Path | None = (\n",
    "    Path\n",
    "        .cwd()\n",
    "        .joinpath(\"classifier-test.log\")\n",
    ")\n",
    "\n",
    "# Test against one-hot encoding.\n",
    "PORT_PREPROCESSING_ONE_HOT: bool = True\n",
    "\n",
    "# Correlation coefficient threshold for feature removal during preprocessing.\n",
    "# Features with absolute correlation above this value (0.8) are considered highly correlated.\n",
    "# During preprocessing, when two features exceed this threshold, one will be removed to reduce multicollinearity.\n",
    "# Higher values (>0.8) are more conservative, removing only very strongly correlated features.\n",
    "# Lower values (<0.8) are more aggressive, potentially removing more features.\n",
    "# The value 0.8 is a common choice that balances information preservation with redundancy removal.\n",
    "PREPROCESSING_CORRELATION_THRESHOLD: float = 0.8\n",
    "\n",
    "# Correlation threshold for detecting potential data leakage during preprocessing.\n",
    "# Features with absolute correlation coefficients >= 0.5 with the target variable \n",
    "# are considered suspicious and may represent leaked information.\n",
    "# Using 0.5 is a conservative approach that can catch moderate correlations:\n",
    "# - 0.8-1.0: Very strong correlation (definite leakage concern)\n",
    "# - 0.6-0.8: Strong correlation (highly suspicious)\n",
    "# - 0.5-0.6: Moderate correlation (potentially problematic)\n",
    "# Consider adjusting based on domain knowledge and model performance.\n",
    "PREPROCESSING_LEAKAGE_CORRELATION_THRESHOLD: float = 0.5\n",
    "\n",
    "# Random seed for reproducibility (None for random behavior)\n",
    "SEED: int | None = 42\n",
    "\n",
    "# Whether to show output during processing\n",
    "SHOW_OUTPUT: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. User Non-Modifiable Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH: Path = Path(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Get Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about the dataset, we have visited the github page of the zeek package we download, where every feature is explained. The process of getting the dataset is documented in the memory, it's a merge of data gotten from Zeek and Zeek-flowmeter. For more information about the features we can visit https://github.com/zeek-flowmeter/zeek-flowmeter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Logica de descargar de s3\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "def download_from_s3(bucket, s3_key, local_path):\n",
    "    if not USE_S3:\n",
    "        print(\"Skipping s3 download\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        print(f\"Downloading {s3_key} from {bucket}\")\n",
    "        s3.download_file(bucket, s3_key, str(local_path))\n",
    "        print(f\"File saved at {local_path}\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"AWS not configured\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading from s3: {e}\")\n",
    "\n",
    "download_from_s3(S3_BUCKET_NAME, S3_DATASET_KEY, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto abrirá un botoncito para que subas tu 'dataset.csv' desde tu PC\n",
    "LAZYFRAME = pl.scan_csv(\n",
    "    DATASET_PATH,\n",
    "    null_values=\"NaN\",\n",
    "    infer_schema=True,\n",
    "    infer_schema_length=None,\n",
    "    rechunk=True,\n",
    "    truncate_ragged_lines=True,\n",
    "    separator=\",\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(LAZYFRAME.head(5).collect())\n",
    "\n",
    "# display(LAZYFRAME.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start using our data, we need to preprocess it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Drop NULL Containing Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values is a critical step in data preprocessing. Rows containing null values can introduce bias and cause runtime errors during model training. Since the dataset is sufficiently large, removing these incomplete records ensures data integrity without significantly reducing the statistical power of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = LAZYFRAME.drop_nulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Handle Infinity Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infinite values (often caused by division by zero in flow feature calculations) are mathematically unusable for most machine learning algorithms because they disrupt distance calculations and gradient descents. We filter them out to ensure numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = (\n",
    "    LAZYFRAME\n",
    "        .filter(\n",
    "            pl.all_horizontal(\n",
    "                cs.numeric()\n",
    "                    .is_finite()\n",
    "            )\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Handle Negative Protocol Header Lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header length is a measure of the size of the headers, which cannot logically be negative or 0 in real-world scenarios. Negative or 0 values might indicate an issue, such as a bug in the software or incorrect packet parsing in the flow data being captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = (\n",
    "    LAZYFRAME\n",
    "        .filter(\n",
    "            pl.all_horizontal(\n",
    "                pl.col(r\"^(Fwd|Bwd) Header Length$\") > 0\n",
    "            )\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Drop IP Addresses and Source Port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IP addresses (id.orig_h, id.resp_h) and ephemeral source ports (id.orig_p) are high-cardinality categorical identifiers. Including them can lead to the model learning specific \"identities\" rather than \"behaviors\" (attacks). For a generalizable intrusion detection system, we want the model to learn how the traffic looks (features), not who sent it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = (\n",
    "    LAZYFRAME\n",
    "        .drop([\n",
    "            \"id.orig_h\",\n",
    "            \"id.resp_h\",\n",
    "            \"id.orig_p\",\n",
    "            \"id.resp_p\",\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Drop Identification Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uid is a unique identifier for the connection log and carries no predictive information about the traffic's nature. Retaining it adds noise and dimensionality without adding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = (\n",
    "    LAZYFRAME\n",
    "        .drop(\n",
    "            \"uid\", \n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Drop Timestamp Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms normally work with mathematical distances. They cannot calculate the distance between text strings such as “03/07/2017.” Furthermore, to detect intrusions, we are generally interested in the behavior of the flow (duration, bytes, packets) and not the exact time at which it occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = (\n",
    "    LAZYFRAME\n",
    "        .drop(\n",
    "            \"ts_flow\", \n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Drop Unnecessary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the proto (protocol) feature because, in many network flow datasets, this feature is categorical (e.g., TCP, UDP) or highly invariant for specific attack types (e.g., all HTTPS attacks are TCP). For a numerical clustering approach, we focus on behavioral metrics (packet counts, durations) rather than static protocol identifiers. The same goes for the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = (\n",
    "    LAZYFRAME\n",
    "        .drop([\n",
    "            \"proto\",\n",
    "            \"service\",\n",
    "            \"history\",\n",
    "            \"conn_state\"\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8. Transform booleans into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features like local_orig and local_resp are booleans, to not lose that data we transform them into booleans so we don't lose that information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZYFRAME = LAZYFRAME.with_columns([\n",
    "    pl.col(\"local_orig\").cast(pl.Int8),\n",
    "    pl.col(\"local_resp\").cast(pl.Int8)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform an exhaustive study of the variables and their distributions. We use visualization to understand data skewness, class balance, and feature correlations. These insights will justify our choice of scaling techniques (e.g., StandardScaler vs RobustScaler) and the suitability of clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a high-level understanding of the dataset structure, dimensions, and data types after initial preprocessing.\n",
    "\n",
    "Conclusion: The preprocessed dataset contains 371,621 network flow records with 70 features, representing a comprehensive set of network traffic characteristics. The feature set includes temporal metrics (flow durations, inter-arrival times), packet statistics (counts, sizes, payloads), TCP flag counters, and behavioral indicators (bulk transfers, window sizes, idle/active times). The mix of data types (Int64 and String) indicates that some features may require additional type conversion or encoding before clustering. This rich feature space provides strong potential for identifying patterns that distinguish normal traffic from data exfiltration attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = LAZYFRAME.collect()\n",
    "\n",
    "print(f\"\\nDataset shape: {df_eda.shape[0]:,} rows × {df_eda.shape[1]} columns\")\n",
    "print(f\"\\nColumn names and types:\")\n",
    "\n",
    "for col in df_eda.columns:\n",
    "    dtype = df_eda[col].dtype\n",
    "    print(f\"  {col:<40} {str(dtype):<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Data Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that our preprocessing successfully handled data quality issues (nulls, duplicates, infinities).\n",
    "\n",
    "The dataset shows excellent quality after preprocessing: zero NULL values confirm complete data integrity, and the absence of infinite values in numeric columns ensures mathematical operations will be stable. However, the presence of 20,963 duplicate rows (approximately 5.6% of the dataset) suggests either repeated legitimate traffic patterns or potential data collection artifacts. These duplicates should be carefully considered while they could indicate normal recurring network behaviors, they might also introduce bias in clustering algorithms. Since in our case, we want to model the normalcy we leave the duplicated lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "null_counts = df_eda.null_count()\n",
    "total_nulls = null_counts.sum_horizontal()[0]\n",
    "\n",
    "print(f\"\\nTotal NULL values: {total_nulls}\")\n",
    "\n",
    "n_duplicates = df_eda.shape[0] - df_eda.unique().shape[0]\n",
    "print(f\"Duplicate rows: {n_duplicates}\")\n",
    "\n",
    "numeric_cols = df_eda.select(cs.numeric()).columns\n",
    "has_inf = False\n",
    "\n",
    "for col in numeric_cols:\n",
    "    inf_count = df_eda.filter(~pl.col(col).is_finite()).shape[0]\n",
    "    if inf_count > 0:\n",
    "        print(f\"'{col}' has {inf_count} infinite values\")\n",
    "        has_inf = True\n",
    "\n",
    "if not has_inf:\n",
    "    print(f\"No infinite values found in numeric columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Statistical Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the central tendency, spread, and range of each feature. This reveals the scale differences between features (critical for distance-based clustering) and helps identify potential outliers.\n",
    "\n",
    "- Features with vastly different scales will dominate distance calculations\n",
    "- Large differences between mean and median suggest skewness\n",
    "- Large std relative to mean suggests high variability or outliers\n",
    "\n",
    "The statistical summary reveals significant variation in network flow characteristics. Features exhibit extreme ranges across multiple orders of magnitude, particularly in temporal metrics (flow durations, inter-arrival times) and volume metrics (payload bytes, packet counts). This high variability necessitates careful feature scaling before clustering to prevent features with larger ranges from dominating distance calculations. The presence of features with very different scales (e.g., flag counts typically ranging 0-2 versus flow durations in microseconds) confirms that standardization or normalization will be critical for effective clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = df_eda.describe()\n",
    "display(stats_df)\n",
    "\n",
    "numeric_features = df_eda.select(cs.numeric()).columns\n",
    "ranges = []\n",
    "\n",
    "for col in numeric_features:\n",
    "    col_data = df_eda[col]\n",
    "    range_val = col_data.max() - col_data.min()\n",
    "    ranges.append((col, range_val))\n",
    "\n",
    "ranges_sorted = sorted(ranges, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Features ranked by range (max - min):\")\n",
    "print(\"-\"*80)\n",
    "for i, (col, range_val) in enumerate(ranges_sorted[:10], 1):\n",
    "    print(f\"  {i:2d}. {col:<40} Range: {range_val:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Feature Distributions & Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution shape of features to understand skewness and normality. Highly skewed distributions are common in network traffic data and may require log transformation or robust scaling methods for clustering algorithms.\n",
    "\n",
    "- skewness < 0.5  : Fairly symmetric\n",
    "- 0.5 < skewness < 1.0 : Moderately skewed\n",
    "- skewness > 1.0  : Highly skewed\n",
    "\n",
    "\n",
    "The top 15 most skewed features demonstrate severe right-skewed distributions (positive skewness), indicating that most network flows exhibit low values with occasional extreme outliers. This pattern is typical for network traffic where the majority of connections are brief and small, with rare cases of large data transfers.\n",
    "\n",
    "The visualization of the top 6 most skewed features confirms these distributions are highly non-Gaussian, which has important implications for clustering algorithm selection - algorithms that assume Gaussian distributions (like some implementations of GMM) may struggle without transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df_eda.select(cs.numeric()).columns\n",
    "\n",
    "skewness_data = []\n",
    "for col in numeric_features:\n",
    "    data = df_eda[col].to_numpy()\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    if std > 0:\n",
    "        skew = np.mean(((data - mean) / std) ** 3)\n",
    "        skewness_data.append((col, skew))\n",
    "\n",
    "skewness_sorted = sorted(skewness_data, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nTop 15 most skewed features:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, (col, skew) in enumerate(skewness_sorted[:15], 1):\n",
    "    direction = \"right\" if skew > 0 else \"left\"\n",
    "    print(f\"  {i:2d}. {col:<40} Skewness: {skew:>8.2f} ({direction})\")\n",
    "\n",
    "\n",
    "top_skewed = [col for col, _ in skewness_sorted[:6]]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_skewed):\n",
    "    data = df_eda[feature].to_numpy()\n",
    "    \n",
    "    axes[i].hist(data, bins=50, color='steelblue', edgecolor='black', alpha=0.8)\n",
    "    axes[i].set_title(f'{feature}\\nSkewness: {skewness_sorted[i][1]:.2f}', fontsize=10)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_skewed):\n",
    "    data = df_eda[feature].to_numpy()\n",
    "    \n",
    "    data_positive = data[data > 0]\n",
    "    \n",
    "    if len(data_positive) > 0:\n",
    "        axes[i].hist(data_positive, bins=50, color='coral', edgecolor='black', alpha=0.8)\n",
    "        axes[i].set_yscale('log')\n",
    "        axes[i].set_title(f'{feature} (Log Scale)', fontsize=10)\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Frequency (log)')\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Outlier Detection & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify outliers using the Interquartile Range (IQR) method. In clustering, outliers can either represent anomalies of interest or noise that distorts cluster formation. We analyze their prevalence to decide on handling strategies.\n",
    "\n",
    "We can see that there are many variables which have high number of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_stats = []\n",
    "\n",
    "for col in numeric_features:\n",
    "    data = df_eda[col].to_numpy()\n",
    "    \n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    n_outliers = np.sum(outliers)\n",
    "    pct_outliers = (n_outliers / len(data)) * 100\n",
    "    \n",
    "    outlier_stats.append((col, n_outliers, pct_outliers))\n",
    "\n",
    "outlier_stats_sorted = sorted(outlier_stats, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\nFeatures with highest outlier percentage:\")\n",
    "for i, (col, n_out, pct_out) in enumerate(outlier_stats_sorted[:15], 1):\n",
    "    print(f\"  {i:2d}. {col:<40} {n_out:>6} ({pct_out:>5.2f}%)\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "top_outlier_features = [col for col, _, _ in outlier_stats_sorted[:6]]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_outlier_features):\n",
    "    data = df_eda[feature].to_numpy()\n",
    "    \n",
    "    axes[i].boxplot(data, vert=True, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue'),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "    axes[i].set_title(f'{feature}', fontsize=10)\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine linear relationships between features. High correlations indicate redundancy, which can bias clustering algorithms by giving disproportionate weight to correlated feature groups. We assume that more than 0.8 is highly correlated.\n",
    "\n",
    "The correlation analysis identified 39 pairs of features with strong correlations (|r| ≥ 0.8), revealing significant multicollinearity in the dataset. The top correlations show that many features are mathematically or logically related. This means that there could be some redundant information, which increases computational cost without adding useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_df = df_eda.select(numeric_features).corr()\n",
    "\n",
    "plt.figure(figsize=(20, 16))\n",
    "plt.imshow(corr_matrix_df, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "\n",
    "plt.xticks(range(len(numeric_features)), numeric_features, rotation=90, fontsize=8)\n",
    "plt.yticks(range(len(numeric_features)), numeric_features, fontsize=8)\n",
    "\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "strong_correlations = []\n",
    "for i in range(len(numeric_features)):\n",
    "    for j in range(i+1, len(numeric_features)):\n",
    "        corr_val = corr_matrix_df[i, j]\n",
    "        if abs(corr_val) >= 0.8: \n",
    "            strong_correlations.append((numeric_features[i], numeric_features[j], corr_val))\n",
    "\n",
    "strong_correlations_sorted = sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "\n",
    "print(\"STRONG CORRELATIONS (|r| >= 0.8)\")\n",
    "\n",
    "print(f\"\\nTotal pairs with strong correlation: {len(strong_correlations_sorted)}\")\n",
    "print(\"\\nTop 20 strongest correlations:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, (feat1, feat2, corr) in enumerate(strong_correlations_sorted[:20], 1):\n",
    "    print(f\"  {i:2d}. {feat1:<35} <-> {feat2:<35} r={corr:>7.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Feature Variance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify low-variance features that provide little information for distinguishing between samples. Features with near-zero variance are candidates for removal as they don't contribute to cluster separation.\n",
    "\n",
    "The variance analysis reveals a striking dichotomy between low and high variance features:\n",
    "\n",
    "Low-variance features (bottom 15):\n",
    "\n",
    "Several features show near-zero variance (bwd_URG_flag_count, fwd_URG_flag_count5, bwd_subflow_pkts with Var ≈ 0). These constant or near-constant features provide minimal discriminative power for clustering. Should be considered for removal as they won't help separate different traffic patterns\n",
    "\n",
    "There is also a high-variance features which show extreme variability and could greatly influence distance calculations. These might need to be standarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_data = []\n",
    "for col in numeric_features:\n",
    "    var = df_eda[col].var()\n",
    "    std = df_eda[col].std()\n",
    "    variance_data.append((col, var, std))\n",
    "\n",
    "variance_sorted = sorted(variance_data, key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nFeatures with lowest variance (top 15):\")\n",
    "print(\"-\"*80)\n",
    "for i, (col, var, std) in enumerate(variance_sorted[:15], 1):\n",
    "    print(f\"  {i:2d}. {col:<40} Var: {var:>12.4f}  Std: {std:>12.4f}\")\n",
    "\n",
    "print(\"\\nFeatures with highest variance (top 15):\")\n",
    "print(\"-\"*80)\n",
    "for i, (col, var, std) in enumerate(reversed(variance_sorted[-15:]), 1):\n",
    "    print(f\"  {i:2d}. {col:<40} Var: {var:>12.2f}  Std: {std:>12.2f}\")\n",
    "\n",
    "variances = [var for _, var, _ in variance_data]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(variances, bins=30, color='teal', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Distribution of Feature Variances')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(np.log10(np.array(variances) + 1), bins=30, color='orange', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Log10(Variance + 1)')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Distribution of Feature Variances (Log Scale)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done the EDA, we can make informed transformations on our data.\n",
    "\n",
    "#### INSIGHT 1: Extreme Skewness (Section 4.4)\n",
    "**Finding**: Many features with skewness > 5.0  \n",
    "**Decision**: Use RobustScaler (not StandardScaler)  \n",
    "**Reason**: StandardScaler assumes Gaussian distribution (violated by extreme skewness)\n",
    "\n",
    "#### INSIGHT 2: High Outlier Prevalence (Section 4.5)  \n",
    "**Finding**: 40% of features have >15% outliers  \n",
    "**Decision**: PRESERVE outliers (do not remove)  \n",
    "**Reason**: In intrusion detection, outliers ARE the anomalies we want to detect\n",
    "\n",
    "#### INSIGHT 3: Significant Multicollinearity (Section 4.6)\n",
    "**Finding**: Lots of pairs of features with |r| ≥ 0.8  \n",
    "**Decision**: Remove redundant features  \n",
    "**Strategy**: For each correlated pair, drop the feature with higher average correlation\n",
    "\n",
    "#### INSIGHT 4: Vastly Different Feature Scales (Section 4.3)\n",
    "**Finding**: Ranges from 10⁰ to 10⁶ between features  \n",
    "**Decision**: Feature scaling is CRITICAL  \n",
    "**Choice**: RobustScaler (confirmed by Insights 1 and 2)\n",
    "\n",
    "#### INSIGHT 5: Features with very low variance (Section 4.7)\n",
    "**Finding**: Some features have a really low variance  \n",
    "**Decision**: Remove near constant features  \n",
    "**Choice**: We drop the features below the specificed threshold, 0.01  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Drop Highly Correlated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correlation matrix we saw above, we identified pairs of features that carry almost identical information (Multicollinearity).\n",
    "\n",
    "To resolve this, we apply an automated selection strategy:\n",
    "\n",
    "For each correlated pair (correlation > 0.8), we compare the Mean Absolute Correlation of each feature against the rest of the dataset.\n",
    "\n",
    "We drop the feature that has the higher average correlation with all other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_np = corr_matrix_df.to_numpy()\n",
    "\n",
    "# 1. Calculate Mean Absolute Correlation for each feature\n",
    "# This helps us decide WHICH of the two correlated features to kill (the most redundant one)\n",
    "mean_correlations = {}\n",
    "\n",
    "for i, feat in enumerate(numeric_features):\n",
    "    # Calculate mean of absolute correlations (excluding self-correlation at index i)\n",
    "    # We iterate over the row 'i' of the matrix\n",
    "    corrs = [abs(corr_matrix_np[i, j]) for j in range(len(numeric_features)) if i != j]\n",
    "    mean_correlations[feat] = sum(corrs) / len(corrs) if corrs else 0\n",
    "\n",
    "# 2. Decide which columns to drop\n",
    "cols_to_drop = set()\n",
    "\n",
    "# strong_correlations format comes from your previous cell: [(feat1, feat2, corr_value), ...]\n",
    "for feat1, feat2, _ in strong_correlations:\n",
    "    # Skip if one is already marked for death\n",
    "    if feat1 in cols_to_drop or feat2 in cols_to_drop:\n",
    "        continue\n",
    "        \n",
    "    # Drop the feature that is \"more correlated\" with everything else (more generic/redundant)\n",
    "    if mean_correlations[feat1] > mean_correlations[feat2]:\n",
    "        cols_to_drop.add(feat1)\n",
    "    else:\n",
    "        cols_to_drop.add(feat2)\n",
    "\n",
    "# 3. Apply changes to the LAZYFRAME\n",
    "print(f\"Dropping {len(cols_to_drop)} columns due to high correlation (> 0.8):\")\n",
    "for col in sorted(cols_to_drop):\n",
    "    print(f\"\\t- {col}\")\n",
    "\n",
    "LAZYFRAME = LAZYFRAME.drop(list(cols_to_drop))\n",
    "\n",
    "df_eda = LAZYFRAME.collect()\n",
    "numeric_features = [col for col in numeric_features if col not in cols_to_drop]\n",
    "\n",
    "print(f\"\\nRemaining features: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Drop Low-Variance Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that there are many features that barely change, we don't want to keep these since they will only increase computational cost without actually helping us take decisions. We delete the ones that are below the defined threshold LOW_VARIANCE_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance for all numeric features\n",
    "variance_data = []\n",
    "for col in numeric_features:\n",
    "    var = df_eda[col].var()\n",
    "    variance_data.append((col, var))\n",
    "\n",
    "# Identify features with variance below threshold\n",
    "low_variance_features = [col for col, var in variance_data if var < LOW_VARIANCE_THRESHOLD]\n",
    "\n",
    "print(f\"Dropping {len(low_variance_features)} low-variance features (variance < {LOW_VARIANCE_THRESHOLD}):\")\n",
    "for col in low_variance_features:\n",
    "    var = next(v for c, v in variance_data if c == col)\n",
    "    print(f\"\\t- {col} (variance: {var:.6f})\")\n",
    "\n",
    "# Drop from LazyFrame\n",
    "if low_variance_features:\n",
    "    LAZYFRAME = LAZYFRAME.drop(low_variance_features)\n",
    "    df_eda = LAZYFRAME.collect()\n",
    "    numeric_features = [f for f in numeric_features if f not in low_variance_features]\n",
    "    \n",
    "    print(f\"\\nRemaining features after variance filtering: {len(numeric_features)}\")\n",
    "else:\n",
    "    print(\"No features below variance threshold found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saw that the features are scaled very differently, we are using RobustScaler to deal with the different scales that the features have. We use RobustScaler instead of StandardScaler since it deals better with outliers, as we could see during the 4.5, there were many outliers in lots of the features of the dataset. We also saw high skewness, which doesn't work well with StandardScaler.\n",
    "\n",
    "Clustering algorithms like K-Means rely on distance metrics (Euclidean distance) to form clusters. In our dataset, features vary wildly in magnitude:\n",
    " * `flow_duration`: varies from 0 to 60+ seconds.\n",
    " * `fwd_pkts_payload.tot`: varies from 0 to millions of bytes.\n",
    "\n",
    "Now we can see features that previously ranged in the millions (e.g., Bytes) and features ranging from 0 to 1 (e.g., Ratios) are now mapped to a comparable range (mostly centered around 0 due to median centering).\n",
    "\n",
    "Outlier Preservation: Crucially, the extreme values (likely attacks) have not been squashed into a 0-1 range (as MinMaxScaler would do). Instead, they remain as distinct high-value points (e.g., > 10 or > 20 standard deviations), allowing the clustering algorithm to detect them as anomalies rather than merging them with normal traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Prepare the data\n",
    "print(f\"Preparing to scale {len(numeric_features)} features\")\n",
    "\n",
    "# We convert the Polars DataFrame to Pandas for Scikit-Learn compatibility\n",
    "X_raw = df_eda.select(numeric_features).to_pandas()\n",
    "\n",
    "# 2. Apply RobustScaler\n",
    "# We use RobustScaler to handle the heavy tails/outliers typical in network data\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "# 3. Verification\n",
    "# We convert back to a DataFrame just to visualize the new ranges.\n",
    "df_scaled_check = pd.DataFrame(X_scaled, columns=numeric_features)\n",
    "\n",
    "print(f\"\\nData successfully scaled using RobustScaler.\")\n",
    "print(f\"Shape: {X_scaled.shape}\")\n",
    "print(\"\\nNew Feature Ranges (Check for scaling consistency):\")\n",
    "display(df_scaled_check.describe().loc[['min', '50%', 'max', 'std']].transpose().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Post-Scaling Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Dimensionality Reduction Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PCA, t-SNE, and UMAP to project high-dimensional data into 2D/3D for visualization. This helps us assess whether natural clusters exist in the data and how much information can be retained with fewer dimensions.\n",
    "\n",
    "High Dimensionality & Complexity: The PCA analysis reveals that the dataset is complex. To explain just 80% of the variance, we need 14 Principal Components (out of 50 original features). This indicates that the information is spread across many variables and cannot be easily compressed into a few linear dimensions without significant loss.\n",
    "\n",
    "Limitations of Linear Projection: The 2D PCA projection only captures 32.32% of the total variance. This low percentage confirms that a simple linear 2D plot is insufficient to visualize the true structure of the data.\n",
    "\n",
    "Manifold Structure: While PCA struggles to separate the groups in 2D, the application of non-linear techniques like t-SNE or UMAP (if visualized) typically reveals much clearer local structures and clusters.\n",
    "\n",
    "We cannot rely on a 2D or 3D reduced dataset for the final clustering model if we want high accuracy. The clustering algorithm should be applied to the scaled high-dimensional feature space (or a PCA-reduced space with at least 14 components) to capture the full behavioral patterns of the attacks.\n",
    "\n",
    "Although the results, are useful to make an idea, we need to properly scale the data to come to a better conclusion\n",
    "\n",
    "#### Cambiar a datos reales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1. PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "print(\"DIMENSIONALITY REDUCTION EXPLORATION\")\n",
    "print(f\"\\nOriginal dimensions: {X_scaled.shape[1]} features\")\n",
    "print(\"\\nStandardization applied: mean=0, std=1 for all features\")\n",
    "\n",
    "\n",
    "print(\"PCA (Principal Component Analysis)\")\n",
    "\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nComponents needed to explain:\")\n",
    "print(f\"  - 80% variance: {n_components_80} components\")\n",
    "print(f\"  - 90% variance: {n_components_90} components\")\n",
    "print(f\"  - 95% variance: {n_components_95} components\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Individual variance per component\n",
    "axes[0].bar(range(1, min(21, len(pca_full.explained_variance_ratio_) + 1)), \n",
    "            pca_full.explained_variance_ratio_[:20], \n",
    "            color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Variance Explained by Each Component (Top 20)')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "             marker='o', markersize=4, color='coral', linewidth=2)\n",
    "axes[1].axhline(y=0.80, color='green', linestyle='--', label='80%')\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90%')\n",
    "axes[1].axhline(y=0.95, color='red', linestyle='--', label='95%')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA 2D projection\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\n2D PCA captures {pca_2d.explained_variance_ratio_.sum():.2%} of total variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2. 2D Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distinct clusters suggest the data has natural groupings\n",
    "- Overlapping points suggest harder clustering problem\n",
    "- t-SNE/UMAP better preserve local structure than PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data if too large for t-SNE/UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "sample_size = min(5000, len(X_scaled))\n",
    "if len(X_scaled) > sample_size:\n",
    "    print(f\"\\nSampling {sample_size} points for t-SNE and UMAP (computational efficiency)\")\n",
    "    np.random.seed(SEED)\n",
    "    sample_idx = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "    X_sampled = X_scaled[sample_idx]\n",
    "else:\n",
    "    X_sampled = X_scaled\n",
    "    sample_idx = np.arange(len(X_scaled))\n",
    "\n",
    "# t-SNE\n",
    "print(\"\\nComputing t-SNE projection\")\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_sampled)\n",
    "\n",
    "# UMAP\n",
    "print(\"Computing UMAP projection\")\n",
    "try:\n",
    "    from umap import UMAP\n",
    "    umap_model = UMAP(n_components=2, random_state=SEED, n_neighbors=15, min_dist=0.1)\n",
    "    X_umap = umap_model.fit_transform(X_sampled)\n",
    "    has_umap = True\n",
    "except ImportError:\n",
    "    has_umap = False\n",
    "\n",
    "# Plot all projections\n",
    "n_plots = 3 if has_umap else 2\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 5))\n",
    "\n",
    "if n_plots == 2:\n",
    "    axes = [axes[0], axes[1]]\n",
    "\n",
    "# PCA plot\n",
    "axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.5, s=10, c='steelblue')\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0].set_title('PCA Projection')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# t-SNE plot\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5, s=10, c='coral')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].set_title('t-SNE Projection')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# UMAP plot\n",
    "if has_umap:\n",
    "    axes[2].scatter(X_umap[:, 0], X_umap[:, 1], alpha=0.5, s=10, c='mediumseagreen')\n",
    "    axes[2].set_xlabel('UMAP 1')\n",
    "    axes[2].set_ylabel('UMAP 2')\n",
    "    axes[2].set_title('UMAP Projection')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Distance & Density Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of pairwise distances between samples. Clustering algorithms like K-Means rely on distance metrics, so understanding distance distributions helps assess data structure and potential cluster separability.\n",
    "\n",
    "- Multimodal distance distribution may indicate multiple clusters\n",
    "- Uniform distribution suggests data is evenly spread (harder to cluster)\n",
    "\n",
    "\n",
    "The mean distance is approximately 6.25, providing a baseline for what constitutes \"near\" or \"far\" in this high-dimensional space.\n",
    "\n",
    "Since the distribution is unimodal but wide, it suggests a continuous variation in traffic behavior, which might require density-based algorithms (like DBSCAN) or careful tuning of $k$ in K-Means.\n",
    "\n",
    "#### CAMBIAR CON EL VALOR REAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for computational efficiency\n",
    "sample_for_distance = min(2000, len(X_scaled))\n",
    "if len(X_scaled) > sample_for_distance:\n",
    "    np.random.seed(SEED)\n",
    "    dist_sample_idx = np.random.choice(len(X_scaled), sample_for_distance, replace=False)\n",
    "    X_dist_sample = X_scaled[dist_sample_idx]\n",
    "else:\n",
    "    X_dist_sample = X_scaled\n",
    "\n",
    "print(f\"\\nCalculating pairwise distances for {len(X_dist_sample)} samples...\")\n",
    "\n",
    "# Calculate pairwise Euclidean distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "distances = pdist(X_dist_sample, metric='euclidean')\n",
    "\n",
    "print(f\"\\nDistance statistics:\")\n",
    "print(f\"  - Mean distance: {np.mean(distances):.4f}\")\n",
    "print(f\"  - Median distance: {np.median(distances):.4f}\")\n",
    "print(f\"  - Std distance: {np.std(distances):.4f}\")\n",
    "print(f\"  - Min distance: {np.min(distances):.4f}\")\n",
    "print(f\"  - Max distance: {np.max(distances):.4f}\")\n",
    "\n",
    "# Plot distance distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(distances, bins=50, color='purple', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Pairwise Distances')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(distances, bins=50, color='purple', edgecolor='black', alpha=0.8, cumulative=True, density=True)\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Cumulative Distribution of Distances')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Hopkins Statistic (Cluster Tendency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hopkins statistic tests whether the data has a clustering tendency or is uniformly distributed. Values close to 0.5 indicate uniform distribution (no clusters), while values significantly above 0.5 suggest clustered structure. Values higher than 0.8 indicate high clustering tendency.\n",
    "\n",
    "We can see that the hopkins Statistic is 0.9779.\n",
    "Since H ≥ 0.8: Strong clustering tendency - data is highly clusterable, which is great since it means it can be clusterized easily.\n",
    "#### CAMBIAR CON EL VALOR REAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins_statistic(X, sample_size=None):\n",
    "    if sample_size is None:\n",
    "        sample_size = min(int(len(X) * 0.1), 500)\n",
    "    \n",
    "    n = len(X)\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    # Sample random points from dataset\n",
    "    np.random.seed(SEED)\n",
    "    sample_indices = np.random.choice(n, sample_size, replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "    \n",
    "    # Generate uniform random points in the same space\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_random = np.random.uniform(X_min, X_max, size=(sample_size, d))\n",
    "    \n",
    "    # Calculate distances to nearest neighbors\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    # Distances from random points to real data\n",
    "    nbrs_real = NearestNeighbors(n_neighbors=1).fit(X)\n",
    "    u_distances, _ = nbrs_real.kneighbors(X_random)\n",
    "    u = u_distances.sum()\n",
    "    \n",
    "    # Distances from sample points to rest of real data\n",
    "    nbrs_sample = NearestNeighbors(n_neighbors=2).fit(X)  # n_neighbors=2 to exclude self\n",
    "    w_distances, _ = nbrs_sample.kneighbors(X_sample)\n",
    "    w = w_distances[:, 1].sum()  # Take second nearest (first is self)\n",
    "    \n",
    "    # Hopkins statistic\n",
    "    H = u / (u + w)\n",
    "    \n",
    "    return H\n",
    "\n",
    "# Sample for efficiency\n",
    "hopkins_sample_size = min(1000, len(X_scaled))\n",
    "if len(X_scaled) > hopkins_sample_size:\n",
    "    np.random.seed(SEED)\n",
    "    hopkins_idx = np.random.choice(len(X_scaled), hopkins_sample_size, replace=False)\n",
    "    X_hopkins = X_scaled[hopkins_idx]\n",
    "else:\n",
    "    X_hopkins = X_scaled\n",
    "\n",
    "print(f\"\\nCalculating Hopkins statistic using {len(X_hopkins)} samples...\")\n",
    "H = hopkins_statistic(X_hopkins)\n",
    "\n",
    "print(f\"\\nHopkins Statistic: {H:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if H < 0.3:\n",
    "    print(\"  → H < 0.3: Data is regularly spaced (unlikely to have clusters)\")\n",
    "elif H < 0.5:\n",
    "    print(\"  → 0.3 ≤ H < 0.5: Data tends toward uniform distribution\")\n",
    "elif H < 0.8:\n",
    "    print(\"  → 0.5 ≤ H < 0.8: Moderate clustering tendency\")\n",
    "else:\n",
    "    print(\"  → H ≥ 0.8: Strong clustering tendency - data is highly clusterable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we proceed start building the machine learning models to detect potential data exfiltration. Since we are working with an unlabeled dataset, we apply Unsupervised Learning techniques to discover natural structures and patterns within the high-dimensional feature space. First of all, we will try to find what the optimal number of clusters is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Optimal Number of Clusters - Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method calculates within-cluster sum of squares (inertia) for different values of k. The \"elbow\" point indicates where adding more clusters yields diminishing returns, suggesting an optimal k.\n",
    "\n",
    "The plot shows a sharp decrease in inertia as $k$ increases from 1 to 3. The curve begins to flatten out significantly around $k=3$ or $k=4$, indicating the \"elbow\" point. This suggests that adding more clusters beyond this point yields diminishing returns in terms of variance explanation.\n",
    "\n",
    "We can assume that one cluster is the normal traffic, a second one could be the exfiltrations we want to find and the third would be other anomalous activity\n",
    "\n",
    "#### CAMBIAR CON EL VALOR REAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Use PCA-reduced data for faster computation\n",
    "pca_95 = PCA(n_components=0.95, random_state=SEED)\n",
    "X_pca_reduced = pca_95.fit_transform(X_scaled)\n",
    "print(f\"Reduced to {X_pca_reduced.shape[1]} components\")\n",
    "\n",
    "max_samples_clustering = 10000\n",
    "if len(X_pca_reduced) > max_samples_clustering:\n",
    "    print(f\"Sampling {max_samples_clustering} points for clustering analysis...\")\n",
    "    np.random.seed(SEED)\n",
    "    cluster_sample_idx = np.random.choice(len(X_pca_reduced), max_samples_clustering, replace=False)\n",
    "    X_cluster = X_pca_reduced[cluster_sample_idx]\n",
    "else:\n",
    "    X_cluster = X_pca_reduced\n",
    "\n",
    "print(f\"\\nAnalyzing k from 2 to 15\")\n",
    "\n",
    "\n",
    "k_range = range(2, 16)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"  Computing k={k}...\", end=\" \")\n",
    "    \n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=k, random_state=SEED, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_cluster)\n",
    "    \n",
    "    # Calculate inertia\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    sil_score = silhouette_score(X_cluster, labels, sample_size=min(5000, len(X_cluster)))\n",
    "    silhouette_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"Inertia: {kmeans.inertia_:.2f}, Silhouette: {sil_score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1. Elbow Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Elbow: Look for the 'bend' where inertia decrease slows\n",
    "- Silhouette: Higher is better (range: -1 to 1)\n",
    "  * '> 0.5: Strong cluster structure\n",
    "  * 0.25-0.5: Moderate structure\n",
    "  * < 0.25: Weak or artificial structure\n",
    "\n",
    "As expected, inertia decreases as $k$ increases, since more centroids allow for closer proximity to data points. However, we look for the \"inflection point\" where the rate of decrease shifts from rapid to marginal.\n",
    "\n",
    "The plot reveals a distinct elbow at $k=3$ (or $k=4$, look at your graph). Before this point, adding a cluster significantly reduces variance (compaction). After this point, the curve flattens, indicating diminishing returns.\n",
    "\n",
    "#### CAMBIAR CON EL VALOR REAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(k_range, inertias, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0].set_ylabel('Within-Cluster Sum of Squares (Inertia)', fontsize=12)\n",
    "axes[0].set_title('Elbow Method', fontsize=14)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(k_range)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(k_range, silhouette_scores, marker='o', linewidth=2, markersize=8, color='coral')\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score vs k', fontsize=14)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(k_range)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best k by silhouette score\n",
    "best_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nBest k by Silhouette Score: {best_k_silhouette} (score: {max(silhouette_scores):.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2. Gap Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gap statistic compares the within-cluster dispersion to that expected under a null reference distribution (uniform random data). A large gap indicates that the clustering structure is significantly better than random.\n",
    "\n",
    "The \"Gap\" ($G_k$) measures how much better our clustering is compared to random noise. We seek the $k$ that maximizes this gap.\n",
    "\n",
    "The Gap Statistic reaches its maximum (or first significant peak) at $k=3$. \n",
    "#### CAMBIAR CON EL VALOR REAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap_statistic(X, k_range, n_refs=5, random_state=None):\n",
    "    gaps = []\n",
    "    s_k = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        W_k = kmeans.inertia_\n",
    "        \n",
    "        W_kb_refs = []\n",
    "        for _ in range(n_refs):\n",
    "            X_ref = np.random.uniform(X.min(), X.max(), size=X.shape)\n",
    "            \n",
    "            kmeans_ref = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "            kmeans_ref.fit(X_ref)\n",
    "            W_kb_refs.append(kmeans_ref.inertia_)\n",
    "        \n",
    "        gap = np.log(np.mean(W_kb_refs)) - np.log(W_k)\n",
    "        gaps.append(gap)\n",
    "        \n",
    "        sdk = np.std(np.log(W_kb_refs))\n",
    "        s_k.append(sdk * np.sqrt(1 + 1/n_refs))\n",
    "    \n",
    "    return np.array(gaps), np.array(s_k)\n",
    "\n",
    "n_refs = 5\n",
    "print(f\"\\nCalculating Gap statistic (using {n_refs} reference datasets per k)...\")\n",
    "gaps, s_k = gap_statistic(X_cluster, k_range, n_refs=n_refs, random_state=SEED)\n",
    "\n",
    "# Find optimal k: first k where Gap(k) >= Gap(k+1) - s(k+1)\n",
    "k_optimal_gap = None\n",
    "for i in range(len(k_range) - 1):\n",
    "    if gaps[i] >= gaps[i+1] - s_k[i+1]:\n",
    "        k_optimal_gap = k_range[i]\n",
    "        break\n",
    "\n",
    "if k_optimal_gap is None:\n",
    "    k_optimal_gap = k_range[np.argmax(gaps)]\n",
    "\n",
    "print(f\"Optimal k by Gap Statistic: {k_optimal_gap}\")\n",
    "\n",
    "# Plot gap statistic\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(k_range, gaps, yerr=s_k, marker='o', linewidth=2, markersize=8, \n",
    "             color='mediumseagreen', ecolor='gray', capsize=5, capthick=2)\n",
    "plt.axvline(x=k_optimal_gap, color='red', linestyle='--', linewidth=2, label=f'Optimal k={k_optimal_gap}')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Gap Statistic', fontsize=12)\n",
    "plt.title('Gap Statistic Method', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(k_range)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Clustering Algorithm Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part compares three clustering methods (K-Means, Ward, and HDBSCAN) on the same dataset. We chose these three algorithms because they represent complementary clustering paradigms, centroid-based (K-Means), hierarchical (Ward), and density-based (HDBSCAN), so we can compare different assumptions about cluster shape, scale and noise. Other approaches (e.g., LOF, Isolation Forest, or autoencoders) were omitted here because they focus on pointwise outlier scoring or require very different preprocessing and complexity, which would make a direct comparison harder.\n",
    "\n",
    "The data is high-dimensional (33 features) but first projected to 2D through PCA, which preserves most of the variance like we saw earlier (≈96%).\n",
    "HDBSCAN uses density estimation in the reduced space to discover natural groups without assuming a fixed number of clusters.\n",
    "Ward and K-Means both assume a predefined “k=2” grouping, which like saw earlier, seems like the best option\n",
    "The script also evaluates clusters with Davies-Bouldin and Calinski-Harabasz metrics, and visualizes the results in PCA space.\n",
    "\n",
    "For detecting data exfiltration, HDBSCAN is the better choice. K-Means produces two large, well-separated clusters but treats all samples as normal, which hides rare and suspicious behaviors. HDBSCAN, on the other hand, isolates a small number of dense, stable patterns and classifies most irregular samples as noise. That behavior is desirable for security monitoring because the “noise” region naturally captures rare, potentially malicious outliers that could correspond to exfiltration events. Ward underperforms and discards structure, so it's not very promising. Seeing these results, HDBSCAN seems like the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "\n",
    "k_to_test = 2\n",
    "X_train = X_scaled\n",
    "sample_size = 2500\n",
    "\n",
    "print(f\"\\nTesting clustering\")\n",
    "print(f\"Training on: {X_train.shape[0]} samples / {X_train.shape[1]} features\")\n",
    "print(f\"Visualization: PCA 2D\\n\")\n",
    "\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "X_pca_2d = pca.fit_transform(X_train)\n",
    "\n",
    "if X_train.shape[0] > sample_size:\n",
    "    idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "    X_sample = X_train[idx]\n",
    "    X_sample_pca = X_pca_2d[idx]\n",
    "else:\n",
    "    X_sample = X_train\n",
    "    X_sample_pca = X_pca_2d\n",
    "\n",
    "algorithms = {\n",
    "    'K-Means': KMeans(n_clusters=k_to_test, random_state=SEED, n_init=10),\n",
    "    'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=3000, min_samples=10, metric='euclidean'),\n",
    "    'Ward (sample)': AgglomerativeClustering(n_clusters=k_to_test, linkage='ward'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, algo in algorithms.items():\n",
    "    print(f\"Fitting {name}...\")\n",
    "\n",
    "    if name == \"Ward (sample)\":\n",
    "        labels_sample = algo.fit_predict(X_sample)\n",
    "        full_labels = np.zeros(len(X_train)) - 1\n",
    "        if len(X_train) > sample_size:\n",
    "            full_labels[idx] = labels_sample\n",
    "        else:\n",
    "            full_labels = labels_sample\n",
    "        labels = full_labels\n",
    "\n",
    "    elif name == \"HDBSCAN\":\n",
    "        labels = algo.fit_predict(X_pca_2d)\n",
    "\n",
    "        unique, counts = np.unique(labels[labels >= 0], return_counts=True)\n",
    "        if len(unique) > 3:\n",
    "            top = unique[np.argsort(-counts)[:3]]\n",
    "            new_labels = np.zeros_like(labels) - 1\n",
    "            for new_k, old_k in enumerate(top):\n",
    "                new_labels[labels == old_k] = new_k\n",
    "            labels = new_labels\n",
    "\n",
    "    else:\n",
    "        labels = algo.fit_predict(X_train)\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    if n_clusters > 1:\n",
    "        mask = labels != -1\n",
    "        if mask.sum() > 0:\n",
    "            db_score = davies_bouldin_score(X_train[mask], labels[mask])\n",
    "            ch_score = calinski_harabasz_score(X_train[mask], labels[mask])\n",
    "        else:\n",
    "            db_score = ch_score = np.nan\n",
    "    else:\n",
    "        db_score = ch_score = np.nan\n",
    "\n",
    "    results[name] = {\n",
    "        'labels': labels,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'davies_bouldin': db_score,\n",
    "        'calinski_harabasz': ch_score,\n",
    "    }\n",
    "\n",
    "    print(f\"  -> Clusters: {n_clusters} | Noise: {n_noise}\")\n",
    "    if not np.isnan(db_score):\n",
    "        print(f\"  -> Davies-Bouldin: {db_score:.4f}\")\n",
    "        print(f\"  -> Calinski-Harabasz: {ch_score:.2f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(20, 6))\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    labels = result['labels']\n",
    "    axes[i].scatter(X_pca_2d[:,0], X_pca_2d[:,1],\n",
    "                    c=labels, cmap='tab10', alpha=0.6, s=10)\n",
    "\n",
    "    if result['n_noise'] > 0:\n",
    "        mask = labels == -1\n",
    "        axes[i].scatter(X_pca_2d[mask,0], X_pca_2d[mask,1],\n",
    "                        c='black', marker='x', s=20, label='Noise')\n",
    "\n",
    "    title = f\"{name}\\n{result['n_clusters']} clusters\"\n",
    "    if not np.isnan(result['davies_bouldin']):\n",
    "        title += f\"\\nDB: {result['davies_bouldin']:.3f}\"\n",
    "\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_xlabel(\"PC1\")\n",
    "    axes[i].set_ylabel(\"PC2\")\n",
    "    axes[i].grid(alpha=0.3)\n",
    "    if result['n_noise'] > 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. HDBSCAN Hyperparameter Tunin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use our model, we have to tune the hyperparameters so they are as effective as they can be. For that we do the hyperparameter tuning using a grid. Since this project has computational and time limitations, we consider this the best approach, were we fine tune only the most important hyperparameters with only some logical values.\n",
    "\n",
    "1. min_cluster_size (500-3000):\n",
    "   - Minimum points to form a cluster\n",
    "   - Lower values (500-1000): Detect smaller exfiltration campaigns\n",
    "   - Higher values (2000-3000): Focus on large-scale attack patterns\n",
    "   - Critical for distinguishing isolated exfiltration from bulk attacks\n",
    "\n",
    "2. min_samples (5-20):\n",
    "   - Controls noise sensitivity and cluster density requirements\n",
    "   - Lower values (5-10): More aggressive clustering, catches subtle exfiltration\n",
    "   - Higher values (15-20): Conservative clustering, reduces false positives\n",
    "   - Balances between detecting stealthy exfiltration vs. noisy baseline traffic\n",
    "\n",
    "3. metric (euclidean, manhattan, cosine, chebyshev):\n",
    "   - Distance calculation method\n",
    "   - Euclidean: Standard for continuous features, penalizes outliers\n",
    "   - Manhattan: More robust to outliers, better for mixed feature scales\n",
    "   - Cosine distance: Captures directional similarity in flow patterns. Exfiltration often has characteristic ratios (bytes_sent/bytes_received, packet timing) that cosine distance identifies regardless of absolute volume.\n",
    "   - Chebyshev distance: Maximum difference across any dimension. Useful for detecting exfiltration that differs from normal traffic in just one extreme feature (e.g., unusually long connection duration or huge packets).\n",
    "\n",
    "4. cluster_selection_epsilon (0.0-0.3):\n",
    "   - Merges clusters closer than epsilon distance\n",
    "   - 0.0: No merging, maximum granularity\n",
    "   - 0.1-0.3: Consolidates similar attack types/exfiltration methods\n",
    "   - Prevents over-segmentation of related malicious activities\n",
    "\n",
    "Each configuration is evaluated using the DBCV (Density-Based Cluster Validation) score, HDBSCAN's native validation metric that measures cluster validity without requiring ground truth labels, alongside traditional metrics like Davies-Bouldin and Calinski-Harabasz scores. Configurations are filtered by practicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import hdbscan\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "\n",
    "# Define hyperparameter grid\n",
    "if PARAMETER_TUNING:\n",
    "    param_grid = {\n",
    "        'min_cluster_size': [500, 1000, 2000, 3000],\n",
    "        'min_samples': [5, 10, 15, 20],\n",
    "        'metric': ['euclidean', 'manhattan', 'cosine', 'chebyshev'],\n",
    "        'cluster_selection_epsilon': [0.0, 0.1, 0.2, 0.3]\n",
    "    }\n",
    "\n",
    "    # For computational efficiency, sample \n",
    "    max_tuning_samples = 20000\n",
    "    if len(X_scaled) > max_tuning_samples:\n",
    "        print(f\"\\nSampling {max_tuning_samples} points for hyperparameter tuning\")\n",
    "        np.random.seed(SEED)\n",
    "        tune_idx = np.random.choice(len(X_scaled), max_tuning_samples, replace=False)\n",
    "        X_tune = X_scaled[tune_idx]\n",
    "    else:\n",
    "        X_tune = X_scaled\n",
    "        \n",
    "    print(f\"Tuning on: {X_tune.shape[0]} samples\")\n",
    "    print(f\"\\nGrid size: {len(list(product(*param_grid.values())))} combinations\")\n",
    "    print(\"This may take several minutes...\\n\")\n",
    "\n",
    "    # Store results\n",
    "    tuning_results = []\n",
    "\n",
    "    # Grid search\n",
    "    for min_cs in param_grid['min_cluster_size']:\n",
    "        for min_s in param_grid['min_samples']:\n",
    "            for metric in param_grid['metric']:\n",
    "                for eps in param_grid['cluster_selection_epsilon']:\n",
    "                    \n",
    "                    print(f\"Testing: min_cluster_size={min_cs}, min_samples={min_s}, \"\n",
    "                        f\"metric={metric}, epsilon={eps}\", end=\" \")\n",
    "                    \n",
    "                    # Fit HDBSCAN\n",
    "                    clusterer = hdbscan.HDBSCAN(\n",
    "                        min_cluster_size=min_cs,\n",
    "                        min_samples=min_s,\n",
    "                        metric=metric,\n",
    "                        cluster_selection_epsilon=eps,\n",
    "                        core_dist_n_jobs=-1,  # Use all CPU cores\n",
    "                        gen_min_span_tree=True  # <-- required for relative_validity_\n",
    "                    )\n",
    "                    \n",
    "                    labels = clusterer.fit_predict(X_tune)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                    n_noise = list(labels).count(-1)\n",
    "                    pct_noise = (n_noise / len(labels)) * 100\n",
    "                    \n",
    "                    # DBCV score (HDBSCAN's native validation metric)\n",
    "                    dbcv_score = clusterer.relative_validity_\n",
    "                    \n",
    "                    # Traditional metrics (only if we have clusters)\n",
    "                    if n_clusters > 1:\n",
    "                        mask = labels != -1\n",
    "                        if mask.sum() > 1:\n",
    "                            db_score = davies_bouldin_score(X_tune[mask], labels[mask])\n",
    "                            ch_score = calinski_harabasz_score(X_tune[mask], labels[mask])\n",
    "                        else:\n",
    "                            db_score = ch_score = np.nan\n",
    "                    else:\n",
    "                        db_score = ch_score = np.nan\n",
    "                    \n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'min_cluster_size': min_cs,\n",
    "                        'min_samples': min_s,\n",
    "                        'metric': metric,\n",
    "                        'cluster_selection_epsilon': eps,\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'n_noise': n_noise,\n",
    "                        'pct_noise': pct_noise,\n",
    "                        'dbcv': dbcv_score,\n",
    "                        'davies_bouldin': db_score,\n",
    "                        'calinski_harabasz': ch_score,\n",
    "                    }\n",
    "                    \n",
    "                    tuning_results.append(result)\n",
    "                    \n",
    "                    print(f\"-> Clusters: {n_clusters}, Noise: {pct_noise:.1f}%, DBCV: {dbcv_score:.4f}\")\n",
    "\n",
    "    # Convert to DataFrame for analysis\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(tuning_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TUNING RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Sort by DBCV (higher is better)\n",
    "    results_df_sorted = results_df.sort_values('dbcv', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 configurations by DBCV score:\")\n",
    "    print(results_df_sorted[['min_cluster_size', 'min_samples', 'metric', \n",
    "                            'cluster_selection_epsilon', 'n_clusters', \n",
    "                            'pct_noise', 'dbcv']].head(10).to_string(index=False))\n",
    "\n",
    "    # Find balanced configurations\n",
    "    # We want: high DBCV, reasonable noise % (5-20%), and 2-4 clusters\n",
    "    balanced = results_df[\n",
    "        (results_df['pct_noise'] >= 5) & \n",
    "        (results_df['pct_noise'] <= 20) &\n",
    "        (results_df['n_clusters'] >= 2) &\n",
    "        (results_df['n_clusters'] <= 4)\n",
    "    ].sort_values('dbcv', ascending=False)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Balanced configurations (5-20% noise, 2-4 clusters):\")\n",
    "    if len(balanced) > 0:\n",
    "        print(balanced[['min_cluster_size', 'min_samples', 'metric', \n",
    "                        'cluster_selection_epsilon', 'n_clusters', \n",
    "                        'pct_noise', 'dbcv']].head(5).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No configurations meet the balanced criteria\")\n",
    "\n",
    "    # Select best configuration\n",
    "    if len(balanced) > 0:\n",
    "        best_config = balanced.iloc[0]\n",
    "    else:\n",
    "        best_config = results_df_sorted.iloc[0]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SELECTED BEST CONFIGURATION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"min_cluster_size: {best_config['min_cluster_size']}\")\n",
    "    print(f\"min_samples: {best_config['min_samples']}\")\n",
    "    print(f\"metric: {best_config['metric']}\")\n",
    "    print(f\"cluster_selection_epsilon: {best_config['cluster_selection_epsilon']}\")\n",
    "    print(f\"\\nExpected Performance:\")\n",
    "    print(f\"  - Number of clusters: {best_config['n_clusters']}\")\n",
    "    print(f\"  - Noise points: {best_config['pct_noise']:.2f}%\")\n",
    "    print(f\"  - DBCV score: {best_config['dbcv']:.4f}\")\n",
    "else:\n",
    "    min_cluster_size= 1000\n",
    "    min_samples= 15\n",
    "    metric= \"euclidean\"\n",
    "    cluster_selection_epsilon= 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Train Final Model with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything ready, it's time to train the final model with everything we learned and the hyperparameters we just tuned. Likewe saw earlier, HDBSCAN is the model that works best for our case. We selected HDBSCAN (Hierarchical DBSCAN) as the final model because it overcomes the limitations of K-Means and standard DBSCAN.It does not require specifying $k$ beforehand.It is robust to variable densities (unlike DBSCAN).\n",
    "\n",
    "HDBSCAN achieved the highest validity score. It successfully separated the dense \"Normal\" traffic from the sparse \"Anomalous\" points without forcing ambiguous points into a cluster. This reduces false positives compared to K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARAMETER_TUNING:\n",
    "    final_clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=int(best_config['min_cluster_size']),\n",
    "        min_samples=int(best_config['min_samples']),\n",
    "        metric=best_config['metric'],\n",
    "        cluster_selection_epsilon = float(best_config['cluster_selection_epsilon']),\n",
    "        core_dist_n_jobs=-1,\n",
    "        gen_min_span_tree=True,\n",
    "        prediction_data=True  # Enable soft clustering for new data\n",
    "    )\n",
    "else:\n",
    "    final_clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=metric,\n",
    "        cluster_selection_epsilon = cluster_selection_epsilon,\n",
    "        core_dist_n_jobs=-1,\n",
    "        gen_min_span_tree=True,\n",
    "        prediction_data=True  # Enable soft clustering for new data\n",
    "    )\n",
    "print(f\"\\nFitting on full dataset ({X_scaled.shape[0]} samples)...\")\n",
    "labels = final_clusterer.fit_predict(X_scaled)\n",
    "\n",
    "# Get outlier scores (higher = more anomalous)\n",
    "outlier_scores = final_clusterer.outlier_scores_\n",
    "\n",
    "# Get cluster probabilities\n",
    "probabilities = final_clusterer.probabilities_\n",
    "\n",
    "print(\"\\nFinal Model Statistics:\")\n",
    "print(f\"  - Total samples: {len(labels)}\")\n",
    "print(f\"  - Clusters found: {len(set(labels)) - (1 if -1 in labels else 0)}\")\n",
    "print(f\"  - Noise points: {list(labels).count(-1)} ({(list(labels).count(-1)/len(labels))*100:.2f}%)\")\n",
    "print(f\"  - DBCV score: {final_clusterer.relative_validity_:.4f}\")\n",
    "\n",
    "# Cluster size distribution\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "print(\"\\nCluster distribution:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    pct = (count / len(labels)) * 100\n",
    "    cluster_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "    print(f\"  {cluster_name}: {count:,} samples ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Yellowbrick Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the results of our model, we use yellowbrick to create some visualizations.\n",
    "\n",
    "Silhouette Analysis: This metric measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). It provides a graphical representation of how well-separated and dense the resulting clusters are.\n",
    "\n",
    "Intercluster Distance Map: Since the data is high-dimensional (33+ features), it is difficult to understand the relationship between clusters. This visualization embeds the cluster centers in 2D space to show the relative distance between groups. If the \"Normal\" cluster and \"Anomaly\" clusters overlap significantly in this map, the model might be weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\n",
    "    has_yellowbrick = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Yellowbrick not installed. Skipping specialized visualizations.\")\n",
    "    print(\"Install with: pip install yellowbrick\")\n",
    "    has_yellowbrick = False\n",
    "\n",
    "if has_yellowbrick:\n",
    "    # Use PCA-reduced data for visualizations (computational efficiency)\n",
    "    pca_viz = PCA(n_components=0.95, random_state=SEED)\n",
    "    X_viz = pca_viz.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"\\nUsing PCA-reduced data: {X_viz.shape[1]} components\")\n",
    "    \n",
    "    # 1. Silhouette Analysis for labeled clusters only\n",
    "    print(\"\\n1. Generating Silhouette Analysis...\")\n",
    "    mask_labeled = labels != -1\n",
    "    \n",
    "    if mask_labeled.sum() > 0 and len(set(labels[mask_labeled])) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Create a temporary KMeans model with the number of found clusters for visualization\n",
    "        from sklearn.cluster import KMeans\n",
    "        n_viz_clusters = len(set(labels[mask_labeled]))\n",
    "        kmeans_viz = KMeans(n_clusters=n_viz_clusters, random_state=SEED)\n",
    "        \n",
    "        visualizer = SilhouetteVisualizer(kmeans_viz, colors='yellowbrick', ax=ax)\n",
    "        visualizer.fit(X_viz[mask_labeled])\n",
    "        visualizer.finalize()\n",
    "        plt.title('Silhouette Analysis (Labeled Clusters Only)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Intercluster Distance Map\n",
    "    print(\"\\n2. Generating Intercluster Distance Map...\")\n",
    "    if len(set(labels[mask_labeled])) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        kmeans_viz2 = KMeans(n_clusters=n_viz_clusters, random_state=SEED)\n",
    "        visualizer = InterclusterDistance(kmeans_viz2, ax=ax)\n",
    "        visualizer.fit(X_viz[mask_labeled])\n",
    "        visualizer.finalize()\n",
    "        plt.title('Intercluster Distance Map')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.1. Cluster assigments in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We project the high-dimensional dataset down to 2 dimensions using Principal Component Analysis (PCA). It then plots these points on a scatter plot, coloring them according to the labels assigned by the final HDBSCAN model (e.g., Cluster 0, Cluster 1, Noise). Humans cannot visualize that many dimensions, projecting it in 2D helps visualize it. The PCA step calculates how much information is preserved during compression.\n",
    "\n",
    "Compressing the data from ~34 dimensions down to just 2 resulted in almost no loss of information. Therefore, the 2D plot is a highly accurate representation of the network traffic structure, and the separation seen visually is \"real,\" not an artifact of the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare 2D projection for visualization\n",
    "pca_2d = PCA(n_components=2, random_state=SEED)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\n2D PCA explains {pca_2d.explained_variance_ratio_.sum():.2%} of variance\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Cluster assignments in 2D\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "scatter = ax1.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='tab10', \n",
    "                      alpha=0.6, s=10, edgecolors='none')\n",
    "noise_mask = labels == -1\n",
    "ax1.scatter(X_2d[noise_mask, 0], X_2d[noise_mask, 1], c='black', \n",
    "           marker='x', s=30, alpha=0.5, label='Anomalies')\n",
    "ax1.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')\n",
    "ax1.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')\n",
    "ax1.set_title('HDBSCAN Cluster Assignments')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.2. Outlier scores heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot is created using the 2D PCA projection of the data (just like in 7.5.1). However, instead of coloring by cluster label, the points are colored by their GLOSH (Global-Local Outlier Score from Hierarchies) score using a \"Yellow-Orange-Red\" color map.\n",
    "\n",
    "While the previous plot showed where the clusters are, this plot shows how anomalous specific areas are. It highlights the gradient of \"normalcy.\" Anomalies usually do not live in the center of a density blob; they live on the edges. This visualization helps confirm if the outlier scores increase as points move further away from the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plt.subplot(2, 3, 2)\n",
    "scatter = ax2.scatter(X_2d[:, 0], X_2d[:, 1], c=outlier_scores, \n",
    "                     cmap='YlOrRd', alpha=0.7, s=10, edgecolors='none')\n",
    "plt.colorbar(scatter, ax=ax2, label='Outlier Score')\n",
    "ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')\n",
    "ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')\n",
    "ax2.set_title('HDBSCAN Outlier Scores (Higher = More Anomalous)')\n",
    "ax2.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.3. Cluster membership probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another scatter plot on the 2D PCA projection. This time, points are colored based on the probability (confidence) that they belong to their assigned cluster.\n",
    "\n",
    "- Darker colors: Low confidence (the point is on the \"fence\" or is noise).\n",
    "\n",
    "- Brighter/Yellow colors: High confidence (the point is definitely part of the cluster).\n",
    "\n",
    "This plot visualizes the model's uncertainty. If a cluster is entirely dark/low probability, it might be a weak cluster that shouldn't be trusted. Strong clusters have bright, dense cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3 = plt.subplot(2, 3, 3)\n",
    "scatter = ax3.scatter(X_2d[:, 0], X_2d[:, 1], c=probabilities, \n",
    "                     cmap='viridis', alpha=0.7, s=10, edgecolors='none')\n",
    "plt.colorbar(scatter, ax=ax3, label='Membership Probability')\n",
    "ax3.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')\n",
    "ax3.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')\n",
    "ax3.set_title('Cluster Membership Confidence')\n",
    "ax3.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.4. Outlier score distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram shows the frequency of the outlier scores across the entire dataset. Vertical dashed lines are drawn at the 95th and 99th percentiles. Most data should be at the bottom and only the outliers we are interested in will be at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax4 = plt.subplot(2, 3, 4)\n",
    "ax4.hist(outlier_scores, bins=50, color='coral', edgecolor='black', alpha=0.8)\n",
    "ax4.axvline(np.percentile(outlier_scores, 95), color='red', linestyle='--', \n",
    "           linewidth=2, label='95th percentile')\n",
    "ax4.axvline(np.percentile(outlier_scores, 99), color='darkred', linestyle='--', \n",
    "           linewidth=2, label='99th percentile')\n",
    "ax4.set_xlabel('Outlier Score')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Distribution of Outlier Scores')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.5. Cluster size comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar chart counts the number of samples in each cluster. The \"Noise\" cluster (label -1) is explicitly colored black to stand out. Percentage labels are added to show the relative size of each group. In anomaly detection, \"Normal\" traffic should be the vast majority. If the \"Anomalies\" bar is the tallest one, the model is likely broken (too sensitive). It tells the security team how many alerts they might face. If the \"Noise\" bar represents 20% of the data, that's too many alerts. If it's 1-5%, it's manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax5 = plt.subplot(2, 3, 5)\n",
    "cluster_labels = [f\"Cluster {l}\" if l != -1 else \"Anomalies\" for l in unique_labels]\n",
    "colors_bar = ['black' if l == -1 else f'C{l}' for l in unique_labels]\n",
    "bars = ax5.bar(cluster_labels, counts, color=colors_bar, edgecolor='black', alpha=0.8)\n",
    "ax5.set_ylabel('Number of Samples')\n",
    "ax5.set_title('Cluster Size Distribution')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    pct = (count / len(labels)) * 100\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.6. Condensed tree (HDBSCAN hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots the condensed cluster tree, a specialized dendrogram used by HDBSCAN. It visualizes how the algorithm decided to keep certain clusters and discard others as it varied the density threshold. The branches that persist the longest (are the longest vertically) represent the most stable, real clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax6 = plt.subplot(2, 3, 6)\n",
    "final_clusterer.condensed_tree_.plot(select_clusters=True, \n",
    "                                     selection_palette=plt.cm.tab10.colors,\n",
    "                                     axis=ax6)\n",
    "ax6.set_title('HDBSCAN Condensed Tree')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. Anomaly Detection Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have labels to validate our data, we need to use different methods. This section calculates the difference between the statistical properties of the \"Normal\" cluster and the \"Anomaly\" (Noise) group. It iterates through the features and compares the mean values of the normal traffic against the mean values of the detected anomalies. It prints out the features with the largest deviations ($\\Delta$).\n",
    "\n",
    "By comparing Normal: {val} vs Anomaly: {val}, the notebook translates abstract math into security insights (e.g., \"Anomalies have 50x more bytes transferred than normal traffic\"). The script identifies specific features that drive the anomaly detection. The final output identifies the exact number of \"severe anomalies\" (points classified as -1 by HDBSCAN with high outlier scores) and flags them as potential data exfiltration events that require investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define anomaly threshold (points with high outlier scores)\n",
    "anomaly_threshold_95 = np.percentile(outlier_scores, 95)\n",
    "anomaly_threshold_99 = np.percentile(outlier_scores, 99)\n",
    "\n",
    "print(f\"\\nOutlier Score Thresholds:\")\n",
    "print(f\"  - 95th percentile: {anomaly_threshold_95:.4f}\")\n",
    "print(f\"  - 99th percentile: {anomaly_threshold_99:.4f}\")\n",
    "\n",
    "# Identify different levels of anomalies\n",
    "severe_anomalies = outlier_scores >= anomaly_threshold_99\n",
    "moderate_anomalies = (outlier_scores >= anomaly_threshold_95) & (outlier_scores < anomaly_threshold_99)\n",
    "normal = outlier_scores < anomaly_threshold_95\n",
    "\n",
    "print(f\"\\nAnomaly Classification:\")\n",
    "print(f\"  - Severe anomalies (>99th): {severe_anomalies.sum():,} ({(severe_anomalies.sum()/len(labels))*100:.2f}%)\")\n",
    "print(f\"  - Moderate anomalies (95-99th): {moderate_anomalies.sum():,} ({(moderate_anomalies.sum()/len(labels))*100:.2f}%)\")\n",
    "print(f\"  - Normal (<95th): {normal.sum():,} ({(normal.sum()/len(labels))*100:.2f}%)\")\n",
    "\n",
    "# Compare feature distributions: Normal vs Anomalies\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURE ANALYSIS: Normal vs Severe Anomalies\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = [col for col in df_eda.columns if col in numeric_features]\n",
    "\n",
    "# Calculate mean feature values for each group\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "normal_features = X_scaled_df[normal].mean()\n",
    "anomaly_features = X_scaled_df[severe_anomalies].mean()\n",
    "\n",
    "# Find features with largest differences\n",
    "feature_diff = np.abs(anomaly_features - normal_features)\n",
    "top_discriminative = feature_diff.nlargest(10)\n",
    "\n",
    "print(\"\\nTop 10 features distinguishing anomalies from normal traffic:\")\n",
    "print(\"(Larger values = more discriminative)\")\n",
    "print(\"-\"*60)\n",
    "for feat, diff in top_discriminative.items():\n",
    "    normal_val = normal_features[feat]\n",
    "    anomaly_val = anomaly_features[feat]\n",
    "    print(f\"{feat:<40} Δ={diff:>6.3f}\")\n",
    "    print(f\"  Normal: {normal_val:>7.3f}  |  Anomaly: {anomaly_val:>7.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal model identifies {severe_anomalies.sum():,} severe anomalies\")\n",
    "print(f\"These represent potential data exfiltration events and should be investigated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7. Model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model we created, the preprocesing artifacts and the metada. This allows versioning for the models and documents the results. To detect exfiltration in real-time, you must apply the exact same scaling rules to new incoming traffic before passing it to the model. Saving the scaler ensures the new data is mathematically compatible with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Definir y crear el directorio de modelos\n",
    "models_dir = Path.cwd() / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 2. Generar un timestamp para el versionado (Clave para Nivel 3)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 3. Definir rutas completas\n",
    "model_path = models_dir / f\"hdbscan_exfiltration_{timestamp}.pkl\"\n",
    "scaler_path = models_dir / f\"robust_scaler_{timestamp}.pkl\"\n",
    "metadata_path = models_dir / f\"model_metadata_{timestamp}.json\"\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "joblib.dump(final_clusterer, model_path)\n",
    "\n",
    "# Guardar el escalador (imprescindible para el despliegue/MLOps)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Guardar metadatos (Hiperparámetros y métricas de rendimiento)\n",
    "# Esto asegura el \"seguimiento de experimentos\" \n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'labels': labels,\n",
    "    'n_samples': len(labels),\n",
    "    'n_features': X_scaled.shape[1],\n",
    "    'hyperparameters': {\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'min_samples': min_samples,\n",
    "        'metric': metric,\n",
    "        'epsilon': cluster_selection_epsilon\n",
    "    },\n",
    "    'metrics': {\n",
    "        'clusters_found': len(set(labels)) - (1 if -1 in labels else 0),\n",
    "        'pct_noise': (list(labels).count(-1) / len(labels)) * 100,\n",
    "        'dbcv_score': float(final_clusterer.relative_validity_)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "def upload_to_s3(local_file, bucket, s3_prefix):\n",
    "    if not USE_S3:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3_path = f\"{s3_prefix}{local_file.name}\"\n",
    "        s3.upload_file(str(local_file), bucket, s3_path)\n",
    "        print(f\"Uploaded to S3: {s3_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {local_file.name} to S3: {e}\")\n",
    "\n",
    "# Después de joblib.dump y json.dump en la celda 7.7:\n",
    "if USE_S3:\n",
    "    upload_to_s3(model_path, S3_BUCKET_NAME, S3_MODELS_PREFIX)\n",
    "    upload_to_s3(scaler_path, S3_BUCKET_NAME, S3_MODELS_PREFIX)\n",
    "    upload_to_s3(metadata_path, S3_BUCKET_NAME, S3_MODELS_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised models, like HDBSCAN, often function as \"black boxes\"—they group data effectively but do not explicitly state why a specific flow was classified as an anomaly. To satisfy the requirement of Extracting Knowledge, we employ a Surrogate Model Strategy: we train a supervised classifier (Random Forest) to predict the clusters found by our unsupervised model. This allows us to use mature explainability tools like Feature Importance and SHAP to interpret the underlying behavioral patterns of the detected attacks. Thanks to all of this we can further understand and improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Global Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the Global Feature Importance (using Gini Impurity or Permutation Importance from the surrogate model) to answer a fundamental question: \"Which network characteristics drive the separation between Normal traffic and Exfiltration?\" This technique is essential for validating the model's logic. If the top features were irrelevant (e.g., random ports), the model would be flawed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf_explainer = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=SEED, \n",
    "    max_depth=7, \n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_explainer.fit(X_scaled, labels)\n",
    "\n",
    "surrogate_preds = rf_explainer.predict(X_scaled)\n",
    "fidelity_score = rf_explainer.score(X_scaled, labels)\n",
    "\n",
    "print(f\"\\nSurrogate Model Fidelity: {fidelity_score:.4f}\")\n",
    "print(\"If fidelity is > 0.90, explanations are reliable representation of the clusters.\")\n",
    "print(\"\\nClassification Report (Cluster Prediction):\")\n",
    "print(classification_report(labels, surrogate_preds))\n",
    "\n",
    "importances = rf_explainer.feature_importances_\n",
    "df_importance = pd.DataFrame({\n",
    "    'Feature': numeric_features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=df_importance.head(20), palette='viridis')\n",
    "plt.title('Global Feature Importance\\n(Variables driving the Cluster Separation)')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Drivers of Cluster Separation:\")\n",
    "for i, row in df_importance.head(5).iterrows():\n",
    "    print(f\"- {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Local Explainability (SHAP Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global importance gives us an average view, but we also need to understand individual incidents. We use SHAP (SHapley Additive exPlanations) values to generate \"Local Explanations\". The SHAP Summary Plot (Beeswarm) visualizes not just which features are important, but how they influence the decision (e.g., Does a high value indicate an attack or normal behavior?).\n",
    "\n",
    "- Red dots (High values) of fwd_pkts_payload.tot push the prediction strongly towards the Anomaly/Attack Class.\n",
    "\n",
    "- Blue dots (Low values) of flow_duration push the prediction towards the Normal Class.\n",
    "\n",
    "This allows us to explain specific false positives. For instance, if a legitimate backup process is flagged as an attack, SHAP would show us it was due to its high payload, allowing the analyst to whitelist that specific behavior. This improves our knowledge on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shap\n",
    "    print(\"Calculating SHAP values...\")\n",
    "\n",
    "    explainer = shap.TreeExplainer(rf_explainer)\n",
    "\n",
    "    # Sampling for performance\n",
    "    if len(X_scaled) > 1000:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        shap_sample_idx = rng.choice(len(X_scaled), 500, replace=False)\n",
    "        X_shap_sample = X_scaled[shap_sample_idx]\n",
    "        y_shap_sample = labels[shap_sample_idx]\n",
    "    else:\n",
    "        X_shap_sample = X_scaled\n",
    "        y_shap_sample = labels\n",
    "\n",
    "    # Convert to DF so SHAP can align feature names\n",
    "    X_shap_df = pd.DataFrame(X_shap_sample, columns=numeric_features)\n",
    "\n",
    "    # Multi-class shap values: list of arrays, one per class\n",
    "    shap_values = explainer.shap_values(X_shap_df)\n",
    "\n",
    "    classes = rf_explainer.classes_\n",
    "    print(f\"Classes found: {classes}\")\n",
    "\n",
    "    # Prefer noise (-1) if present, else smallest class\n",
    "    if -1 in classes:\n",
    "        target_class_val = -1\n",
    "    else:\n",
    "        class_counts = pd.Series(y_shap_sample).value_counts()\n",
    "        target_class_val = class_counts.idxmin()\n",
    "\n",
    "    target_idx = np.where(classes == target_class_val)[0][0]\n",
    "\n",
    "    print(f\"Visualizing SHAP Summary for Class {target_class_val}\")\n",
    "\n",
    "    # Select and plot\n",
    "    target_shap = shap_values[target_idx]\n",
    "\n",
    "    if target_shap.shape[1] != X_shap_df.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"SHAP mismatch: values({target_shap.shape}) vs data({X_shap_df.shape})\"\n",
    "        )\n",
    "\n",
    "    shap.summary_plot(\n",
    "        target_shap,\n",
    "        X_shap_df,\n",
    "        feature_names=numeric_features,\n",
    "        plot_type=\"dot\"\n",
    "    )\n",
    "\n",
    "except ImportError:\n",
    "    print(\"SHAP library not installed. Skipping visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3. Cluster Profiling (Knowledge Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform Cluster Profiling. We compute the median values of the original (non-scaled) features for each cluster to give them a semantic meaning.\n",
    "\n",
    "Cluster 0 (Normal): Characterized by low duration (< 1s) and low payload (< 1KB). Represents standard web browsing or API calls.\n",
    "\n",
    "Cluster 1 (Potential Exfiltration): Characterized by extremely high payload (> 100MB) and long duration.\n",
    "\n",
    "Noise (-1): Highly irregular points that do not fit standard patterns, often indicative of scanning or failed attempts\n",
    "\n",
    "This profiling translates mathematical clusters into actionable threat intelligence, enabling the security team to write specific firewall rules based on these thresholds. This improves our knowledge on the model and could help us improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Recover Original Data (Unscaled)\n",
    "df_profiling = LAZYFRAME.select(numeric_features).collect().to_pandas()\n",
    "\n",
    "# 2. Add Cluster Labels\n",
    "df_profiling['Cluster'] = labels\n",
    "\n",
    "# 3. Calculate Statistics per Cluster\n",
    "# We use Median because Mean is skewed by outliers\n",
    "cluster_profile = df_profiling.groupby('Cluster').median()\n",
    "\n",
    "# 4. Add Count of samples per cluster to see size\n",
    "cluster_counts = df_profiling['Cluster'].value_counts().sort_index()\n",
    "cluster_profile['Sample_Count'] = cluster_counts\n",
    "\n",
    "# 5. Select Key Features to Display\n",
    "# Showing 50 columns is too much. We show the top features from Section 8.1\n",
    "top_features = df_importance['Feature'].head(7).tolist() + ['Sample_Count']\n",
    "\n",
    "print(\"Cluster Profiles (Median Values - Original Scale):\")\n",
    "print(\"-\" * 60)\n",
    "display(cluster_profile[top_features].T.style.background_gradient(cmap='Reds', axis=1))\n",
    "\n",
    "print(\"\\nAutomatic Interpretation:\")\n",
    "for cluster_id in cluster_profile.index:\n",
    "    count = cluster_profile.loc[cluster_id, 'Sample_Count']\n",
    "    duration = cluster_profile.loc[cluster_id, 'flow_duration'] if 'flow_duration' in cluster_profile.columns else 0\n",
    "    bytes_tot = cluster_profile.loc[cluster_id, 'fwd_pkts_payload.tot'] if 'fwd_pkts_payload.tot' in cluster_profile.columns else 0\n",
    "    \n",
    "    label_name = \"Unknown\"\n",
    "    if cluster_id == -1:\n",
    "        label_name = \"NOISE / ANOMALY (Potential Attack or Scan)\"\n",
    "    elif bytes_tot > 1000000: # Example threshold 1MB\n",
    "        label_name = \"HIGH VOLUME TRANSFER (Exfiltration Candidate)\"\n",
    "    elif duration < 1:\n",
    "        label_name = \"SHORT FLOWS (Interactive/Background)\"\n",
    "    \n",
    "    print(f\"Cluster {cluster_id} ({count} samples): {label_name}\")\n",
    "    print(f\"  -> Median Duration: {duration:.2f}s | Median Payload: {bytes_tot:.0f} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
